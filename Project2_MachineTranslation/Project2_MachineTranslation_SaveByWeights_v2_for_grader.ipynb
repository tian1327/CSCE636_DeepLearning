{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tianliu_525004380_Project2_MachineTranslation_SaveByWeights_v2_for_grader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "Name: Tian Liu\n",
        "UIN: 525004380"
      ],
      "metadata": {
        "id": "ftHp_pb6zMKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution Explanation:\n",
        "\n",
        "In this project, I implemented the Transformer archetecture for Machine Translation task. Basically, I split the given training data into train and validation set with ratio of 0.75:0.15. Then I tuned the model parameters using the validation set. Finally, the transformer is trained on the entire dataset given. And the final model is then saved and submitted for testing.\n",
        "\n",
        "Notice that the trained model are saved by weights, and loaded by weights for testing. For more instructions on how to run the trained model on testing data, please refer to the laset section `To test the trained model on the testing data` at the end of the script."
      ],
      "metadata": {
        "id": "as-LqlwFjblf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzHKLaH_pBtQ",
        "outputId": "a5bb05ae-4cfd-4f82-cfc7-618b061fc7d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 26 02:02:09 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPhqRD_spRYr",
        "outputId": "ce893634-02dc-418f-f263-ec71a0b30ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd 'drive/MyDrive/Colab Notebooks/CSCE636_DeepLearning/Project2_MachineTranslation/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0PiivhHpTkK",
        "outputId": "66792580-7891-4dd3-dbe7-7ecb9d158460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/CSCE636_DeepLearning/Project2_MachineTranslation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Training Data"
      ],
      "metadata": {
        "id": "n1ChsCdcpaCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "train_input = pickle.load(open('DS_5_train_input', 'rb'))\n",
        "train_output = pickle.load(open('DS_5_train_output', 'rb'))\n",
        "\n",
        "# check dimension\n",
        "print(type(train_input), type(train_input[0]), len(train_input))\n",
        "print(type(train_output), type(train_output[0]), len(train_output))\n",
        "\n",
        "# get the max num of words in train input and output to determine sequence_length\n",
        "# get the max length of a word to determine vocabulary size\n",
        "\n",
        "max_ct = 0\n",
        "max_len = 0\n",
        "vocab_set = set()\n",
        "for text in train_input:\n",
        "  ct = len(text.split())\n",
        "  for word in text.split():\n",
        "    vocab_set.add(word)\n",
        "    length = len(word)\n",
        "    max_len = max(max_len, length)\n",
        "  max_ct = max(max_ct, ct)\n",
        "\n",
        "print('vocab_size in train_input:', len(vocab_set))\n",
        "print('max num of words in train_input:', max_ct)\n",
        "print('max length of a word in train_input:', max_len)\n",
        "print()\n",
        "\n",
        "max_ct = 0\n",
        "max_len = 0\n",
        "vocab_set = set()\n",
        "for text in train_output:\n",
        "  ct = len(text.split())\n",
        "  for word in text.split():\n",
        "    vocab_set.add(word)\n",
        "    length = len(word)\n",
        "    max_len = max(max_len, length)  \n",
        "  max_ct = max(max_ct, ct)\n",
        "\n",
        "print('vocab_size in train_output:', len(vocab_set))\n",
        "print('max num of words in train_output:', max_ct)\n",
        "print('max length of a word in train_output:', max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sblPcAcxprZp",
        "outputId": "713e1d7f-b6a3-4b9b-f714-04531dc09a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> <class 'str'> 5000\n",
            "<class 'list'> <class 'str'> 5000\n",
            "vocab_size in train_input: 11\n",
            "max num of words in train_input: 64\n",
            "max length of a word in train_input: 1\n",
            "\n",
            "vocab_size in train_output: 34\n",
            "max num of words in train_output: 95\n",
            "max length of a word in train_output: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocess Data"
      ],
      "metadata": {
        "id": "reBhJBsFGamk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# insert [start] and [end] to the output language\n",
        "\n",
        "text_pairs = []\n",
        "for i in range(len(train_input)):\n",
        "  input = train_input[i]\n",
        "  output = \"[start] \" + train_output[i].strip() + \" [end]\" # strip off the whitespaces\n",
        "  text_pairs.append((input, output))"
      ],
      "metadata": {
        "id": "E_RsBPj-IiJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check sample text pairs\n",
        "from numpy import random\n",
        "\n",
        "num_sample = 10\n",
        "idx = [random.randint(0, len(train_input)) for i in range(num_sample)]\n",
        "\n",
        "for i in idx:\n",
        "  print(\"idx =\", i)\n",
        "  print(len(text_pairs[i][0].split()), text_pairs[i][0])\n",
        "  print(\"->\")\n",
        "  print(len(text_pairs[i][1].split())-2, text_pairs[i][1])\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOUwxQMb__zQ",
        "outputId": "89177796-52cf-4814-923d-b57d2a72db31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idx = 2349\n",
            "46 a k c g a d a e c e a e a i a f a i c f a k b g c g a e c d a i b g b d c e c g a d c g b g \n",
            "->\n",
            "68 [start] c g c e a e e c f b g c g a k h i c d a e k a i g j l b g b d c e a i ed ee ef a f m eg c g c g b g a d ej ek a i eh ei el a e em a d f fd a k d fe [end]\n",
            "\n",
            "idx = 2034\n",
            "40 a h a d a f c d c f a g b e a i a h b f a d b d c g c e b g a f c e b g c e c g \n",
            "->\n",
            "59 [start] c d c f a f d e b e b f b d c g a d i j c e a h h k l b g c e b g a f ee ef a i m ed eg a g g eh a d f ei c e c g a h ej ek el [end]\n",
            "\n",
            "idx = 610\n",
            "40 a k a e a j a f b d a e c e b f a e a d b d a g a k c e a k b f b e a e b g b e \n",
            "->\n",
            "59 [start] b d c e a e e a f d f b f b d c e b f b e a k k l a k j m b g a e ee a g ed ef a d i eg a e eh a j g h ei a e ej b e a k ek el [end]\n",
            "\n",
            "idx = 2639\n",
            "34 a i c g a j c d a d a j b d a g c e c f c d a f a e b g b f c f b f \n",
            "->\n",
            "50 [start] c g c d b d c e c f a g g h c d a j f i j b g a e l b f a f m ed a d k ee c f a j e ef eg b f a i d eh ei [end]\n",
            "\n",
            "idx = 4822\n",
            "54 a f a h c d a e a f a h b d b f b e b d a h a g c e a e a d c g a j a e c f c e b g a i c f c f c d b e b e \n",
            "->\n",
            "80 [start] c d b d b f b e a h e f g b d a f h i a e j c e c g c f a e ed c e b g a j ee ef eg a d m eh a e ei a g l ej c f c f c d a i el em fd b e a h ek fe ff a h d k fg b e a f fh fi [end]\n",
            "\n",
            "idx = 335\n",
            "42 a f b d a k a e c g a j a e a e c g a k b d b e a h a d b d a k a e c d b d c g c d \n",
            "->\n",
            "62 [start] b d c g a e e c g a e g a e h b d b e a k j k b d c d a e ed b d a k ee ef a d m eg c g c d a h eh ei ej a j i l ek a k f el a f d em [end]\n",
            "\n",
            "idx = 2366\n",
            "46 a f a i a f b g b d a h c f a g a e a j c g b e b d c f c e a i c f a d c d a e c e b d b f \n",
            "->\n",
            "68 [start] b g b d a f d e c f c g b e b d a j h i j a e k c f a g l m c e a h g ed ee c f c d c e a e ei a d eh ej b d a i eg ek el a i f ef em b f a f fd fe [end]\n",
            "\n",
            "idx = 1537\n",
            "42 a d a d c f a d a e a d a f c f b e c e b d a f a k a i c f b f a g c f c d b e b d \n",
            "->\n",
            "62 [start] c f c f b e a f e f c e a d g h a e i b d a d j k a d d l c f b f c f c d a g ef eg a i ed ee eh b e a k ei ej b d a f ek el a d m em [end]\n",
            "\n",
            "idx = 2809\n",
            "52 a k a f a f c e c g a h a h c g a i a k a f b g b f a e b f b f b d c e c e a e a h c d b g c e a e c f \n",
            "->\n",
            "77 [start] c e c g a f d e c g b g b f a f h i b f a e k a k j l b f b d a i m ed ee c e a h g ef eg c e c d b g c e a h ej ek el a e em a h eh ei fd a f f fe c f a e fg a k ff fh [end]\n",
            "\n",
            "idx = 4191\n",
            "46 a f a g b g c g a h a g a d a j b d a g b d c g a d b f b g a k b d c f a k c g b e c f c f \n",
            "->\n",
            "68 [start] b g c g a g d e b d b d c g a g h i b f b g a d k l a j g j m b d c f a k ee ef a d ed eg c g b e a k ei ej a g eh ek c f c f a h el em fd a f f fe [end]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split training data into train and val set"
      ],
      "metadata": {
        "id": "wATDyPV2BkZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - num_val_samples\n",
        "\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:]\n",
        "\n",
        "print(len(train_pairs))\n",
        "print(len(val_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmpPueY1BqxR",
        "outputId": "41156b42-fe0b-4347-ab7e-e1910a073c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4250\n",
            "750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save text_pairs\n",
        "import pickle\n",
        "\n",
        "with open('text_pairs', \"wb\") as out_file:\n",
        "  pickle.dump(text_pairs, out_file)\n",
        "\n",
        "# with open('train_pairs', \"wb\") as out_file:\n",
        "#   pickle.dump(train_pairs, out_file)\n",
        "\n",
        "# with open('val_pairs', \"wb\") as out_file:\n",
        "#   pickle.dump(val_pairs, out_file)"
      ],
      "metadata": {
        "id": "AZ8e3QIMVhYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vectorizing the input and output text pairs"
      ],
      "metadata": {
        "id": "MNy0pg8nNIKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import string\n",
        "import re\n",
        "\n",
        "strip_chars = string.punctuation\n",
        "# print(type(strip_chars))\n",
        "\n",
        "strip_chars = strip_chars.replace(\"[\", \"\") # will keep [start] and [end] as it is to separate the marker from actual word start and end\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\") # replace punctuation characters with the empty string\n",
        "\n",
        "input_vocab_size = 100\n",
        "output_vocab_size = 100\n",
        "\n",
        "sequence_length = 100 # max seq length seen in train data is 95\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=input_vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=output_vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "train_input_texts = [pair[0] for pair in text_pairs] # here use all given training data\n",
        "train_output_texts = [pair[1] for pair in text_pairs]\n",
        "\n",
        "source_vectorization.adapt(train_input_texts)\n",
        "target_vectorization.adapt(train_output_texts)\n",
        "\n",
        "print(len(source_vectorization.get_vocabulary()))\n",
        "print(len(target_vectorization.get_vocabulary()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS53HEPsNDCd",
        "outputId": "81509444-1735-4946-b45c-c3e5c756b3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparing datasets for the translation task"
      ],
      "metadata": {
        "id": "Y55NKNKVYRFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(input, output):\n",
        "    input = source_vectorization(input)\n",
        "    output = target_vectorization(output)\n",
        "    return ({\n",
        "        \"input\": input,\n",
        "        \"output\": output[:, :-1],\n",
        "      }, output[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "\n",
        "print(type(val_ds))\n",
        "\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['input'].shape: {inputs['input'].shape}\")\n",
        "    print(f\"inputs['output'].shape: {inputs['output'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVQDsTO8Ybe2",
        "outputId": "e758acf8-92f9-4fc3-b2ca-a76c0ef2f97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.data.ops.dataset_ops.CacheDataset'>\n",
            "inputs['input'].shape: (64, 100)\n",
            "inputs['output'].shape: (64, 100)\n",
            "targets.shape: (64, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Transformer for Machine Translation"
      ],
      "metadata": {
        "id": "84Z2u9wTZjlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformer Encoder"
      ],
      "metadata": {
        "id": "pHMT4BTwdTyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "BE-hu6tuZp_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using positional encoding to re-inject order information"
      ],
      "metadata": {
        "id": "uX8s8mK6dlWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions # add up the word embedding and position embedding\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "watZVDGUdnZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformer Decoder"
      ],
      "metadata": {
        "id": "6K8UQKA0eUkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ],
      "metadata": {
        "id": "azE53LxieT5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Text Decoder"
      ],
      "metadata": {
        "id": "RDl6W2lfV_4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 100\n",
        "\n",
        "def strip_markers(sentence):\n",
        "  # strip the leading [start] and tailing [end] if existing\n",
        "\n",
        "  words = sentence.split()\n",
        "  if words[0] == \"[start]\":\n",
        "    clean_words = words[1:]\n",
        "  else:\n",
        "    clean_words = words\n",
        "  \n",
        "  if clean_words[-1] == \"[end]\":\n",
        "    clean_words = clean_words[:-1]\n",
        "  else:\n",
        "    clean_words = clean_words\n",
        "\n",
        "  clean_text = \" \".join(clean_words)\n",
        "  return clean_text\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "SJVrO1r9V-no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### End-to-end Transformer"
      ],
      "metadata": {
        "id": "_AUcc_U4el7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 128\n",
        "dense_dim = 512\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"input\")\n",
        "x = PositionalEmbedding(sequence_length, input_vocab_size, embed_dim)(encoder_inputs)\n",
        "# x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x) # stack up transformerencoder layers\n",
        "# x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "# x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"output\")\n",
        "x = PositionalEmbedding(sequence_length, input_vocab_size, embed_dim)(decoder_inputs)\n",
        "# x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(output_vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "ePRujmOGeb0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the sequence-to-sequence Transformer"
      ],
      "metadata": {
        "id": "CUn5wMYkfnoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check how the accuracy metric is calculated in Keras\n",
        "import tensorflow as tf\n",
        "\n",
        "m = tf.keras.metrics.Accuracy()\n",
        "m.update_state([[1,100], [2, 200]], [[1, 200], [3, 300]])\n",
        "m.result().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMk_ABtG4Zst",
        "outputId": "aafa480f-b16e-4e42-fb24-056b92fefec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# turns out the Keras metric acc is very close to the word accuracy, no need for customied callbacks\n",
        "\"\"\"\n",
        "def calculate_word_acc(target_texts, pred_texts):\n",
        "\n",
        "  total_ct = 0\n",
        "  match_ct = 0\n",
        "\n",
        "  assert(len(target_texts)==len(pred_texts)) # assert target sentence number match pred sentence number\n",
        "  \n",
        "  text_pairs = zip(target_texts, pred_texts) \n",
        "  for pair in text_pairs: # loop through each sentence\n",
        "    target = pair[0]\n",
        "    pred = pair[1]\n",
        "\n",
        "    target_words = target.split()\n",
        "    pred_words = pred.split()\n",
        "\n",
        "    total_ct += len(target_words)\n",
        "    min_len = min(len(target_words), len(pred_words))\n",
        "    \n",
        "    for i in range(min_len): # loop through each word\n",
        "      if target_words[i] == pred_words[i]:\n",
        "        match_ct += 1\n",
        "    \n",
        "  word_acc = match_ct / total_ct\n",
        "  return word_acc\n",
        "\n",
        "\n",
        "train_word_acc = []\n",
        "val_word_acc = []\n",
        "\n",
        "# define customized metric as callback function, but very slow\n",
        "class WordAccuracy(keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        self.train_word_acc = 0 \n",
        "        self.val_word_acc = 0 \n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "\n",
        "        # train_word_acc\n",
        "        # train_input_texts = [pair[0] for pair in train_pairs]\n",
        "        # train_output_texts = [strip_markers(pair[1]) for pair in train_pairs]\n",
        "\n",
        "        # train_pred_texts = []\n",
        "        # for input_sentence in train_input_texts:\n",
        "        #   pred_sentence = strip_markers(decode_sequence(input_sentence))\n",
        "        #   train_pred_texts.append(pred_sentence)\n",
        "        # self.train_word_acc = calculate_word_acc(train_output_texts, train_pred_texts)\n",
        "\n",
        "        # val_word_acc\n",
        "        val_input_texts = [pair[0] for pair in val_pairs]\n",
        "        val_output_texts = [strip_markers(pair[1]) for pair in val_pairs]\n",
        "\n",
        "        val_pred_texts = []\n",
        "        for input_sentence in val_input_texts:\n",
        "          pred_sentence = strip_markers(decode_sequence(input_sentence))\n",
        "          val_pred_texts.append(pred_sentence)\n",
        "\n",
        "        self.val_word_acc = calculate_word_acc(val_output_texts, val_pred_texts)\n",
        "\n",
        "        # collect\n",
        "\n",
        "        # train_word_acc.append(self.train_word_acc)\n",
        "        val_word_acc.append(self.val_word_acc)\n",
        "      \n",
        "        print(\"train_word_auc = {:.4f}, val_word_auc = {:.4f}\".format(\\\n",
        "              self.train_word_acc, self.val_word_acc))\n",
        "\"\"\"\n",
        "\n",
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "# word_auc = WordAccuracy()\n",
        "# history = transformer.fit(train_ds, epochs=50, validation_data=val_ds, callbacks=[word_auc])\n",
        "\n",
        "history = transformer.fit(train_ds, epochs=100, validation_data=val_ds, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpVEhXb1fnNp",
        "outputId": "e5ccc1e3-d201-4412-dbea-33b8c8eefdb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "67/67 [==============================] - 12s 67ms/step - loss: 1.6236 - accuracy: 0.2546 - val_loss: 1.1361 - val_accuracy: 0.3468\n",
            "Epoch 2/100\n",
            "67/67 [==============================] - 4s 56ms/step - loss: 1.2196 - accuracy: 0.3308 - val_loss: 1.0865 - val_accuracy: 0.3721\n",
            "Epoch 3/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 1.1052 - accuracy: 0.3720 - val_loss: 0.9702 - val_accuracy: 0.4379\n",
            "Epoch 4/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 1.0404 - accuracy: 0.4008 - val_loss: 0.9783 - val_accuracy: 0.4256\n",
            "Epoch 5/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.9962 - accuracy: 0.4249 - val_loss: 0.9258 - val_accuracy: 0.4493\n",
            "Epoch 6/100\n",
            "67/67 [==============================] - 4s 60ms/step - loss: 0.9523 - accuracy: 0.4492 - val_loss: 0.9217 - val_accuracy: 0.4606\n",
            "Epoch 7/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.9189 - accuracy: 0.4684 - val_loss: 0.8056 - val_accuracy: 0.5193\n",
            "Epoch 8/100\n",
            "67/67 [==============================] - 4s 53ms/step - loss: 0.8839 - accuracy: 0.4854 - val_loss: 0.8220 - val_accuracy: 0.5086\n",
            "Epoch 9/100\n",
            "67/67 [==============================] - 4s 53ms/step - loss: 0.8518 - accuracy: 0.5041 - val_loss: 0.7568 - val_accuracy: 0.5466\n",
            "Epoch 10/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.8229 - accuracy: 0.5195 - val_loss: 0.9215 - val_accuracy: 0.4777\n",
            "Epoch 11/100\n",
            "67/67 [==============================] - 4s 53ms/step - loss: 0.7988 - accuracy: 0.5364 - val_loss: 0.7810 - val_accuracy: 0.5399\n",
            "Epoch 12/100\n",
            "67/67 [==============================] - 4s 53ms/step - loss: 0.7737 - accuracy: 0.5495 - val_loss: 0.6923 - val_accuracy: 0.5725\n",
            "Epoch 13/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.7448 - accuracy: 0.5638 - val_loss: 0.8069 - val_accuracy: 0.5193\n",
            "Epoch 14/100\n",
            "67/67 [==============================] - 4s 57ms/step - loss: 0.7217 - accuracy: 0.5763 - val_loss: 0.6724 - val_accuracy: 0.5929\n",
            "Epoch 15/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.7045 - accuracy: 0.5850 - val_loss: 0.6591 - val_accuracy: 0.5959\n",
            "Epoch 16/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.6824 - accuracy: 0.5978 - val_loss: 0.6102 - val_accuracy: 0.6239\n",
            "Epoch 17/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.6648 - accuracy: 0.6073 - val_loss: 0.5949 - val_accuracy: 0.6348\n",
            "Epoch 18/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.6441 - accuracy: 0.6183 - val_loss: 0.5796 - val_accuracy: 0.6505\n",
            "Epoch 19/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.6199 - accuracy: 0.6316 - val_loss: 0.5791 - val_accuracy: 0.6412\n",
            "Epoch 20/100\n",
            "67/67 [==============================] - 4s 53ms/step - loss: 0.6082 - accuracy: 0.6382 - val_loss: 0.6603 - val_accuracy: 0.6204\n",
            "Epoch 21/100\n",
            "67/67 [==============================] - 4s 53ms/step - loss: 0.5909 - accuracy: 0.6484 - val_loss: 0.5778 - val_accuracy: 0.6502\n",
            "Epoch 22/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.5684 - accuracy: 0.6599 - val_loss: 0.5461 - val_accuracy: 0.6628\n",
            "Epoch 23/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.5511 - accuracy: 0.6685 - val_loss: 0.4910 - val_accuracy: 0.6940\n",
            "Epoch 24/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.5359 - accuracy: 0.6783 - val_loss: 0.5493 - val_accuracy: 0.6643\n",
            "Epoch 25/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.5162 - accuracy: 0.6887 - val_loss: 0.4957 - val_accuracy: 0.6909\n",
            "Epoch 26/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.5053 - accuracy: 0.6964 - val_loss: 0.4800 - val_accuracy: 0.6999\n",
            "Epoch 27/100\n",
            "67/67 [==============================] - 4s 53ms/step - loss: 0.4853 - accuracy: 0.7067 - val_loss: 0.4535 - val_accuracy: 0.7211\n",
            "Epoch 28/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.4733 - accuracy: 0.7127 - val_loss: 0.4404 - val_accuracy: 0.7218\n",
            "Epoch 29/100\n",
            "67/67 [==============================] - 4s 53ms/step - loss: 0.4573 - accuracy: 0.7227 - val_loss: 0.4205 - val_accuracy: 0.7376\n",
            "Epoch 30/100\n",
            "67/67 [==============================] - 4s 53ms/step - loss: 0.4455 - accuracy: 0.7281 - val_loss: 0.5854 - val_accuracy: 0.6748\n",
            "Epoch 31/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.4278 - accuracy: 0.7397 - val_loss: 0.4176 - val_accuracy: 0.7360\n",
            "Epoch 32/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.4194 - accuracy: 0.7450 - val_loss: 0.3957 - val_accuracy: 0.7545\n",
            "Epoch 33/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.3996 - accuracy: 0.7551 - val_loss: 0.4134 - val_accuracy: 0.7478\n",
            "Epoch 34/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.3923 - accuracy: 0.7605 - val_loss: 0.4624 - val_accuracy: 0.7308\n",
            "Epoch 35/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.3816 - accuracy: 0.7665 - val_loss: 0.5692 - val_accuracy: 0.6891\n",
            "Epoch 36/100\n",
            "67/67 [==============================] - 4s 53ms/step - loss: 0.3670 - accuracy: 0.7748 - val_loss: 0.4068 - val_accuracy: 0.7449\n",
            "Epoch 37/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.3656 - accuracy: 0.7770 - val_loss: 0.3717 - val_accuracy: 0.7684\n",
            "Epoch 38/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.3504 - accuracy: 0.7850 - val_loss: 0.4024 - val_accuracy: 0.7546\n",
            "Epoch 39/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.3407 - accuracy: 0.7906 - val_loss: 0.3577 - val_accuracy: 0.7774\n",
            "Epoch 40/100\n",
            "67/67 [==============================] - 4s 53ms/step - loss: 0.3387 - accuracy: 0.7936 - val_loss: 0.3412 - val_accuracy: 0.7863\n",
            "Epoch 41/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.3225 - accuracy: 0.8031 - val_loss: 0.3743 - val_accuracy: 0.7776\n",
            "Epoch 42/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.3134 - accuracy: 0.8091 - val_loss: 0.3177 - val_accuracy: 0.8010\n",
            "Epoch 43/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.3104 - accuracy: 0.8115 - val_loss: 0.2976 - val_accuracy: 0.8152\n",
            "Epoch 44/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.2956 - accuracy: 0.8182 - val_loss: 0.3749 - val_accuracy: 0.7695\n",
            "Epoch 45/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.2933 - accuracy: 0.8215 - val_loss: 0.3042 - val_accuracy: 0.8092\n",
            "Epoch 46/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.2823 - accuracy: 0.8267 - val_loss: 0.3377 - val_accuracy: 0.7931\n",
            "Epoch 47/100\n",
            "67/67 [==============================] - 4s 56ms/step - loss: 0.2805 - accuracy: 0.8299 - val_loss: 0.3618 - val_accuracy: 0.7928\n",
            "Epoch 48/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.2642 - accuracy: 0.8392 - val_loss: 0.2923 - val_accuracy: 0.8220\n",
            "Epoch 49/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.2619 - accuracy: 0.8409 - val_loss: 0.2737 - val_accuracy: 0.8314\n",
            "Epoch 50/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.2600 - accuracy: 0.8433 - val_loss: 0.2847 - val_accuracy: 0.8266\n",
            "Epoch 51/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.2433 - accuracy: 0.8529 - val_loss: 0.2779 - val_accuracy: 0.8294\n",
            "Epoch 52/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.2385 - accuracy: 0.8556 - val_loss: 0.3002 - val_accuracy: 0.8202\n",
            "Epoch 53/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.2413 - accuracy: 0.8561 - val_loss: 0.2495 - val_accuracy: 0.8473\n",
            "Epoch 54/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.2246 - accuracy: 0.8647 - val_loss: 0.2738 - val_accuracy: 0.8359\n",
            "Epoch 55/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.2236 - accuracy: 0.8665 - val_loss: 0.3017 - val_accuracy: 0.8157\n",
            "Epoch 56/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.2169 - accuracy: 0.8694 - val_loss: 0.2905 - val_accuracy: 0.8230\n",
            "Epoch 57/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.2116 - accuracy: 0.8740 - val_loss: 0.3384 - val_accuracy: 0.8224\n",
            "Epoch 58/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.2044 - accuracy: 0.8793 - val_loss: 0.2596 - val_accuracy: 0.8445\n",
            "Epoch 59/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.2033 - accuracy: 0.8793 - val_loss: 0.2472 - val_accuracy: 0.8537\n",
            "Epoch 60/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.1984 - accuracy: 0.8830 - val_loss: 0.2481 - val_accuracy: 0.8499\n",
            "Epoch 61/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1895 - accuracy: 0.8863 - val_loss: 0.2488 - val_accuracy: 0.8524\n",
            "Epoch 62/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.1890 - accuracy: 0.8879 - val_loss: 0.3377 - val_accuracy: 0.8189\n",
            "Epoch 63/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1865 - accuracy: 0.8911 - val_loss: 0.2981 - val_accuracy: 0.8280\n",
            "Epoch 64/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1747 - accuracy: 0.8960 - val_loss: 0.2383 - val_accuracy: 0.8616\n",
            "Epoch 65/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1744 - accuracy: 0.8970 - val_loss: 0.2518 - val_accuracy: 0.8555\n",
            "Epoch 66/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.1701 - accuracy: 0.8996 - val_loss: 0.2334 - val_accuracy: 0.8645\n",
            "Epoch 67/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1673 - accuracy: 0.9027 - val_loss: 0.2271 - val_accuracy: 0.8707\n",
            "Epoch 68/100\n",
            "67/67 [==============================] - 4s 53ms/step - loss: 0.1593 - accuracy: 0.9062 - val_loss: 0.3185 - val_accuracy: 0.8414\n",
            "Epoch 69/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.1610 - accuracy: 0.9074 - val_loss: 0.2633 - val_accuracy: 0.8521\n",
            "Epoch 70/100\n",
            "67/67 [==============================] - 4s 53ms/step - loss: 0.1583 - accuracy: 0.9080 - val_loss: 0.2372 - val_accuracy: 0.8685\n",
            "Epoch 71/100\n",
            "67/67 [==============================] - 4s 53ms/step - loss: 0.1496 - accuracy: 0.9136 - val_loss: 0.2526 - val_accuracy: 0.8622\n",
            "Epoch 72/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1498 - accuracy: 0.9131 - val_loss: 0.2334 - val_accuracy: 0.8716\n",
            "Epoch 73/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1452 - accuracy: 0.9148 - val_loss: 0.2336 - val_accuracy: 0.8710\n",
            "Epoch 74/100\n",
            "67/67 [==============================] - 4s 53ms/step - loss: 0.1421 - accuracy: 0.9189 - val_loss: 0.2982 - val_accuracy: 0.8434\n",
            "Epoch 75/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1436 - accuracy: 0.9176 - val_loss: 0.2486 - val_accuracy: 0.8653\n",
            "Epoch 76/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1345 - accuracy: 0.9222 - val_loss: 0.2855 - val_accuracy: 0.8447\n",
            "Epoch 77/100\n",
            "67/67 [==============================] - 4s 53ms/step - loss: 0.1327 - accuracy: 0.9230 - val_loss: 0.2605 - val_accuracy: 0.8648\n",
            "Epoch 78/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1308 - accuracy: 0.9266 - val_loss: 0.3075 - val_accuracy: 0.8449\n",
            "Epoch 79/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1277 - accuracy: 0.9271 - val_loss: 0.2589 - val_accuracy: 0.8662\n",
            "Epoch 80/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.1252 - accuracy: 0.9284 - val_loss: 0.2662 - val_accuracy: 0.8629\n",
            "Epoch 81/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1213 - accuracy: 0.9308 - val_loss: 0.2233 - val_accuracy: 0.8841\n",
            "Epoch 82/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1191 - accuracy: 0.9326 - val_loss: 0.2214 - val_accuracy: 0.8827\n",
            "Epoch 83/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1191 - accuracy: 0.9344 - val_loss: 0.2737 - val_accuracy: 0.8599\n",
            "Epoch 84/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1152 - accuracy: 0.9341 - val_loss: 0.2269 - val_accuracy: 0.8823\n",
            "Epoch 85/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.1140 - accuracy: 0.9358 - val_loss: 0.2108 - val_accuracy: 0.8887\n",
            "Epoch 86/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.1079 - accuracy: 0.9384 - val_loss: 0.2461 - val_accuracy: 0.8700\n",
            "Epoch 87/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1113 - accuracy: 0.9378 - val_loss: 0.3288 - val_accuracy: 0.8453\n",
            "Epoch 88/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.1067 - accuracy: 0.9408 - val_loss: 0.3426 - val_accuracy: 0.8483\n",
            "Epoch 89/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1027 - accuracy: 0.9428 - val_loss: 0.2282 - val_accuracy: 0.8886\n",
            "Epoch 90/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.0993 - accuracy: 0.9437 - val_loss: 0.2563 - val_accuracy: 0.8741\n",
            "Epoch 91/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1002 - accuracy: 0.9442 - val_loss: 0.2236 - val_accuracy: 0.8844\n",
            "Epoch 92/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.1018 - accuracy: 0.9438 - val_loss: 0.3093 - val_accuracy: 0.8548\n",
            "Epoch 93/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.0911 - accuracy: 0.9499 - val_loss: 0.2802 - val_accuracy: 0.8726\n",
            "Epoch 94/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.0942 - accuracy: 0.9478 - val_loss: 0.2893 - val_accuracy: 0.8671\n",
            "Epoch 95/100\n",
            "67/67 [==============================] - 4s 55ms/step - loss: 0.0947 - accuracy: 0.9477 - val_loss: 0.2175 - val_accuracy: 0.8911\n",
            "Epoch 96/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.0850 - accuracy: 0.9523 - val_loss: 0.3171 - val_accuracy: 0.8651\n",
            "Epoch 97/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.0904 - accuracy: 0.9504 - val_loss: 0.2424 - val_accuracy: 0.8855\n",
            "Epoch 98/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.0845 - accuracy: 0.9534 - val_loss: 0.2533 - val_accuracy: 0.8853\n",
            "Epoch 99/100\n",
            "67/67 [==============================] - 4s 54ms/step - loss: 0.0898 - accuracy: 0.9514 - val_loss: 0.2699 - val_accuracy: 0.8731\n",
            "Epoch 100/100\n",
            "67/67 [==============================] - 4s 53ms/step - loss: 0.0845 - accuracy: 0.9541 - val_loss: 0.2440 - val_accuracy: 0.8862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the trained model\n",
        "transformer.save_weights('tianliu_525004380_project2_transformer_valacc_0.8862.weights') # save by weights\n",
        "transformer.save('tianliu_525004380_project2_transformer_trained_valacc0.8862.model.h5') # save by h5 format\n",
        "transformer.save('tianliu_525004380_project2_transformer_trained_valacc0.8862.model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEJQvJ7LwTqG",
        "outputId": "ec24c5bd-ab57-44b4-a579-2f465fcfb63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, embedding_2_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: tianliu_525004380_project2_transformer_trained_valacc0.8862.model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: tianliu_525004380_project2_transformer_trained_valacc0.8862.model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot the train and val performance"
      ],
      "metadata": {
        "id": "HDlvYm87gk49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot training and validation loss and acc\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "loss_values = history_dict[\"loss\"]\n",
        "val_loss_values = history_dict[\"val_loss\"]\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "plt.plot(epochs, loss_values, \"r\", marker=\"o\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss_values, \"b\", marker=\"o\",label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.clf()\n",
        "acc = history_dict[\"accuracy\"]\n",
        "val_acc = history_dict[\"val_accuracy\"]\n",
        "plt.plot(epochs, acc, \"r\", marker=\"o\", label=\"Training acc\")\n",
        "plt.plot(epochs, val_acc, \"b\", marker=\"o\", label=\"Validation acc\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "8416579QgVB1",
        "outputId": "34ab3868-5fc1-4055-c3a3-d9c79691a95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check translation performance on val dataset"
      ],
      "metadata": {
        "id": "Ua6Rom2lf7E6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample some sentences as test data to see performance\n",
        "\n",
        "test_input_texts = [pair[0] for pair in val_pairs]\n",
        "test_output_texts = [pair[1] for pair in val_pairs]\n",
        "\n",
        "# test_input_texts = [pair[0] for pair in test_pairs]\n",
        "\n",
        "from numpy import random\n",
        "\n",
        "num_sample = 10\n",
        "idx = [random.randint(0, len(val_pairs)) for i in range(num_sample)]\n",
        "\n",
        "for i in idx:\n",
        "  input_sentence = test_input_texts[i]\n",
        "  target = test_output_texts[i]\n",
        "\n",
        "  print(\"idx =\", i)\n",
        "  print(\"-> input\")\n",
        "  print(input_sentence)\n",
        "  print(\"-> target\")\n",
        "  print(strip_markers(target))\n",
        "  print(\"-> predicted\")\n",
        "  print(strip_markers(decode_sequence(input_sentence)))\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xziibla_f6GK",
        "outputId": "68834232-f45e-413a-812c-6ac92b904fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idx = 713\n",
            "-> input\n",
            "a j a f c f c d c e a e a i a f a i a d c f c f a g b g c g c d a e b e a d b g a k b f c d c g \n",
            "-> target\n",
            "c f c d a f d e c e c f c f a d h i b g c g a g k l c d a i j m ed b e a e ef a f ee eg b g b f c d a k ej ek a d ei el c g a i eh em fd a e fe a j f g ff\n",
            "-> predicted\n",
            "c f c d a f d e c e c f c f a d h i b g c g a g k l a i j m b e a f ed ee b e a e eg b g c f a d ei ej a i ef eh ek a e el a e em a j f g fd c g b g a j f ff fg\n",
            "\n",
            "idx = 452\n",
            "-> input\n",
            "a j b g a f a h c f b d a h b g b g a h b f a e c d a g a f b f a k a e c d c f a k a e b e b e c g c d \n",
            "-> target\n",
            "b g c f b d b g b g b f c d a e j b f c d a e m c f a k ed ee a f l ef b e a e eh b e a k ei ej a g eg ek a h i k el a h g h em a h e f fd c g a f fe ff c d a j d fg fh\n",
            "-> predicted\n",
            "b g c f b d b g b g b f c d a e j a h h i k c d a e m a h g l ed c f a e ef c f a k eg eh a g ee ei a h e f ej b e a f ek el b e a f em fd c g a j d fe ff c d a f fg fh\n",
            "\n",
            "idx = 320\n",
            "-> input\n",
            "a j c e a d b g c d a e a f a h c e c d b d a i a d a d b e c f b g b d a e b e \n",
            "-> target\n",
            "c e b g c d a d e f c e c d b d a h h i j b e c f a d l m b g a d ed ee b d b e a e eh a i ef eg ei a f k ej a e ek a j d g el\n",
            "-> predicted\n",
            "c e c d c g a d e f c e c d b d a h i j b e b f a d l m b e a d ed ee b d c e a i ef eg eh a h g ei a e ej a f h ek a j d g el\n",
            "\n",
            "idx = 431\n",
            "-> input\n",
            "a k a e a f c d a d a d a j a j a e c e c g c g c d c g c f c d a d c d a e c d \n",
            "-> target\n",
            "c d c e a e e c g c g a j f g h c d c g a j i j k c f a d l m c d a d ed ee a f d ef a e eg c d c d a e ej a d ei ek a k eh el\n",
            "-> predicted\n",
            "c d c g c f a e f c g c g a j f h i c g c f a j j k l c f a d m ed a d e ee c d c d a e eh a e ei a d eg ej a f ef ek a k d el\n",
            "\n",
            "idx = 644\n",
            "-> input\n",
            "a g a k b f a e a g b d a j c f c g a f a f a g a f a e b g a e c e c g b e c d b e \n",
            "-> target\n",
            "b f b d c f c g b g a e h c e a e j a f i k c g a g l m b e a f ed ee c d a f ef eg a j f g eh a g e ei a e ej a k d ek b e a g el em\n",
            "-> predicted\n",
            "b f b d c f c g b f c g a e h i a f g j c e a e l a g k m c g a e ee a f ed ef b e a f eg eh a j e ei a g e ej a k d ek b e a g el em\n",
            "\n",
            "idx = 272\n",
            "-> input\n",
            "a f a g a j a k a j c e c g c f a e b e b e b f a j a f b e c d a d b g c d a e a h c d c e b d c e \n",
            "-> target\n",
            "c e c g c f a j d e f b e a e h a k g i b e b f a j j k l b e c d a f ed ee b g c d a d eg eh c d c e b d a h ej ek el a e em a j ef ei fd a g m fe c e a f ff fg\n",
            "-> predicted\n",
            "c e c e c f a f e f b e a e h a j d g i b e a k j k b f b e c d a f ed ee b g c d a e eh c d c d a e ek b e a h ei ej el em a e fd a j ef eg fe a j l m ff\n",
            "\n",
            "idx = 196\n",
            "-> input\n",
            "a e a e a d a h a i c e a d c g a d a e c e b d b f a f b g c f b g c f \n",
            "-> target\n",
            "c e c g c e a e f b d a d g h a d e i b f a i d j k b g c f a f m ed b g a h l ee ef c f a d eg eh a e ei a e ej\n",
            "-> predicted\n",
            "c f c d c e a e f a e g a i d e h b f b f a h i j k b f c g a f m ed c g a h l ee ef c f a d eg eh a e ei a e ej\n",
            "\n",
            "idx = 184\n",
            "-> input\n",
            "a h c e b g a h c e a f b g a d b e c f a j c d b g a g a h b e b e c e b f \n",
            "-> target\n",
            "c e b g c e b g b e c f a d h i a f g j c d b g b e b e c e a h ed ee ef b f a g eg eh a j l m ei a h f k ej a h d e ek\n",
            "-> predicted\n",
            "c e c g b e b g c f a e h a f g i c f b d b e a j k l m a d j ed b e c e a h ee ef eg b f a h f eh ei a h d e ej ek a h d e el\n",
            "\n",
            "idx = 264\n",
            "-> input\n",
            "a i a j c d b e c e a g a d c g b e a f b e b d a i a e a i c g b g c d b e a e c g \n",
            "-> target\n",
            "c d b e c e a j d e f c g b e a d h i b e b d a f k l a g j m c g b g c d a i ee ef eg a e eh b e c g a e ek a i ei ej el a i g ed em\n",
            "-> predicted\n",
            "c d c e b e a j d e f c g b e b e a f i j a j d h k b d a e m c g c g b d a i ee ef eg a e eh b e c g a i ei ej ek a i l ed el\n",
            "\n",
            "idx = 490\n",
            "-> input\n",
            "a h a k c f a f a d b d b e b e c e a i b d b d a h a e b f c d a g c e b d \n",
            "-> target\n",
            "c f b d b e a d e f b e a f g h a k d i c e b d b d b f a e ed c d c e b d a g eg eh a h ee ef ei a i l m ej a h j k ek\n",
            "-> predicted\n",
            "c f b d b e a d e f b e a f g h a k d i c d b d b e c f a e ed c d a h m ee ef a i k l eg a k d eh c e b d a h ei ej ek\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrain on the entire given dataset"
      ],
      "metadata": {
        "id": "ZN5FsGRvjPV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_full = make_dataset(text_pairs)"
      ],
      "metadata": {
        "id": "hBI_CPWJkftp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 128\n",
        "dense_dim = 512\n",
        "num_heads = 8\n",
        "vocab_size = 100\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"input\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"output\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "history = transformer.fit(train_ds_full, epochs=80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-f5t5uRvjUwB",
        "outputId": "d35d842f-16e3-4b40-b073-38122190c820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "79/79 [==============================] - 7s 50ms/step - loss: 1.5561 - accuracy: 0.2665\n",
            "Epoch 2/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 1.1760 - accuracy: 0.3450\n",
            "Epoch 3/80\n",
            "79/79 [==============================] - 4s 48ms/step - loss: 1.0691 - accuracy: 0.3889\n",
            "Epoch 4/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 1.0056 - accuracy: 0.4224\n",
            "Epoch 5/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.9532 - accuracy: 0.4502\n",
            "Epoch 6/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.9109 - accuracy: 0.4743\n",
            "Epoch 7/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.8719 - accuracy: 0.4942\n",
            "Epoch 8/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.8358 - accuracy: 0.5150\n",
            "Epoch 9/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.8029 - accuracy: 0.5328\n",
            "Epoch 10/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.7681 - accuracy: 0.5522\n",
            "Epoch 11/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.7423 - accuracy: 0.5650\n",
            "Epoch 12/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.7158 - accuracy: 0.5798\n",
            "Epoch 13/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.6969 - accuracy: 0.5904\n",
            "Epoch 14/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.6702 - accuracy: 0.6036\n",
            "Epoch 15/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.6467 - accuracy: 0.6149\n",
            "Epoch 16/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.6281 - accuracy: 0.6256\n",
            "Epoch 17/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.6045 - accuracy: 0.6385\n",
            "Epoch 18/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.5902 - accuracy: 0.6471\n",
            "Epoch 19/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.5603 - accuracy: 0.6631\n",
            "Epoch 20/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.5455 - accuracy: 0.6714\n",
            "Epoch 21/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.5262 - accuracy: 0.6820\n",
            "Epoch 22/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.5129 - accuracy: 0.6904\n",
            "Epoch 23/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.4911 - accuracy: 0.7028\n",
            "Epoch 24/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.4811 - accuracy: 0.7088\n",
            "Epoch 25/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.4555 - accuracy: 0.7236\n",
            "Epoch 26/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.4394 - accuracy: 0.7308\n",
            "Epoch 27/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.4303 - accuracy: 0.7383\n",
            "Epoch 28/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.4132 - accuracy: 0.7472\n",
            "Epoch 29/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.4011 - accuracy: 0.7546\n",
            "Epoch 30/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.3901 - accuracy: 0.7594\n",
            "Epoch 31/80\n",
            "79/79 [==============================] - 4s 48ms/step - loss: 0.3749 - accuracy: 0.7701\n",
            "Epoch 32/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.3669 - accuracy: 0.7753\n",
            "Epoch 33/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.3551 - accuracy: 0.7822\n",
            "Epoch 34/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.3463 - accuracy: 0.7879\n",
            "Epoch 35/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.3344 - accuracy: 0.7950\n",
            "Epoch 36/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.3272 - accuracy: 0.8007\n",
            "Epoch 37/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.3186 - accuracy: 0.8048\n",
            "Epoch 38/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.3086 - accuracy: 0.8113\n",
            "Epoch 39/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.2993 - accuracy: 0.8161\n",
            "Epoch 40/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.2923 - accuracy: 0.8208\n",
            "Epoch 41/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.2910 - accuracy: 0.8236\n",
            "Epoch 42/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.2795 - accuracy: 0.8300\n",
            "Epoch 43/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.2712 - accuracy: 0.8347\n",
            "Epoch 44/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.2659 - accuracy: 0.8388\n",
            "Epoch 45/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.2572 - accuracy: 0.8431\n",
            "Epoch 46/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.2563 - accuracy: 0.8459\n",
            "Epoch 47/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.2506 - accuracy: 0.8485\n",
            "Epoch 48/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.2411 - accuracy: 0.8539\n",
            "Epoch 49/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.2351 - accuracy: 0.8561\n",
            "Epoch 50/80\n",
            "79/79 [==============================] - 4s 48ms/step - loss: 0.2326 - accuracy: 0.8599\n",
            "Epoch 51/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.2249 - accuracy: 0.8622\n",
            "Epoch 52/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.2213 - accuracy: 0.8665\n",
            "Epoch 53/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.2199 - accuracy: 0.8685\n",
            "Epoch 54/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.2098 - accuracy: 0.8725\n",
            "Epoch 55/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.2059 - accuracy: 0.8755\n",
            "Epoch 56/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.2020 - accuracy: 0.8785\n",
            "Epoch 57/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.1966 - accuracy: 0.8816\n",
            "Epoch 58/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.1918 - accuracy: 0.8854\n",
            "Epoch 59/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.1910 - accuracy: 0.8861\n",
            "Epoch 60/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.1843 - accuracy: 0.8901\n",
            "Epoch 61/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.1783 - accuracy: 0.8932\n",
            "Epoch 62/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.1791 - accuracy: 0.8933\n",
            "Epoch 63/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.1712 - accuracy: 0.8971\n",
            "Epoch 64/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.1666 - accuracy: 0.9000\n",
            "Epoch 65/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.1651 - accuracy: 0.9009\n",
            "Epoch 66/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.1644 - accuracy: 0.9025\n",
            "Epoch 67/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.1600 - accuracy: 0.9057\n",
            "Epoch 68/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.1532 - accuracy: 0.9084\n",
            "Epoch 69/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.1530 - accuracy: 0.9101\n",
            "Epoch 70/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.1516 - accuracy: 0.9116\n",
            "Epoch 71/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.1442 - accuracy: 0.9146\n",
            "Epoch 72/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.1429 - accuracy: 0.9156\n",
            "Epoch 73/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.1438 - accuracy: 0.9163\n",
            "Epoch 74/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.1372 - accuracy: 0.9198\n",
            "Epoch 75/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.1348 - accuracy: 0.9210\n",
            "Epoch 76/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.1293 - accuracy: 0.9240\n",
            "Epoch 77/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.1304 - accuracy: 0.9243\n",
            "Epoch 78/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.1280 - accuracy: 0.9252\n",
            "Epoch 79/80\n",
            "79/79 [==============================] - 4s 49ms/step - loss: 0.1239 - accuracy: 0.9297\n",
            "Epoch 80/80\n",
            "79/79 [==============================] - 4s 50ms/step - loss: 0.1219 - accuracy: 0.9290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the Final Trained Model"
      ],
      "metadata": {
        "id": "nwYx8JCJhfH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.save_weights('tianliu_525004380_project2_transformer_final_0.9290.weights')\n",
        "transformer.save('tianliu_525004380_project2_transformer_final_0.9290.best.h5') # save in h5 format\n",
        "transformer.save('tianliu_525004380_project2_transformer_final_0.9290.best')"
      ],
      "metadata": {
        "id": "cnOjnuJOEP6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "898f10ef-ee77-459a-bfad-d2a84dc7e172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_4_layer_call_fn, embedding_4_layer_call_and_return_conditional_losses, embedding_5_layer_call_fn, embedding_5_layer_call_and_return_conditional_losses, embedding_6_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: tianliu_525004380_project2_transformer_final_0.9290.best/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: tianliu_525004380_project2_transformer_final_0.9290.best/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To test the trained model on the testing data"
      ],
      "metadata": {
        "id": "oDFapwt3husC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set up working directory"
      ],
      "metadata": {
        "id": "s75LU8p_cODl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount your drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwOTYKARay1B",
        "outputId": "ccae3292-3663-4c5c-cd1e-8384533aaa5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change the following directory to your working directory\n",
        "%cd 'drive/MyDrive/Colab Notebooks/CSCE636_DeepLearning/Project2_MachineTranslation/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPyal10fa4gC",
        "outputId": "21575e7b-659b-4863-da4e-d2933b34972e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/CSCE636_DeepLearning/Project2_MachineTranslation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload `trained model` and `text_pairs` to the folder"
      ],
      "metadata": {
        "id": "1TTphPkS1-NY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To load the trained model"
      ],
      "metadata": {
        "id": "ek1tXnsLcTBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the trained model is saved by weights, here we loaded the model by its weights."
      ],
      "metadata": {
        "id": "bDTdiaX2aG6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model by loading the weights\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions # add up the word embedding and position embedding\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        \n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        \n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        \n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        \n",
        "        return self.layernorm_3(attention_output_2 + proj_output)\n",
        "\n",
        "embed_dim = 128\n",
        "dense_dim = 512\n",
        "num_heads = 8\n",
        "vocab_size = 100\n",
        "sequence_length = 100\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"input\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"output\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "transformer_loaded = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "\n",
        "# load the saved weights\n",
        "transformer_loaded.load_weights('tianliu_525004380_project2_transformer_final_0.9290.weights')"
      ],
      "metadata": {
        "id": "HXsPsGURbQ4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85c0f1af-e6eb-471d-e2d9-33c4e6c003de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb010584f10>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To run trained model on test data"
      ],
      "metadata": {
        "id": "ruHExcOi7gFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: before run the trained model to generate translated text for the testing data, the `source_vectorization` and `target_vectorization` have to be initialized by loading the `text_pairs`, such that the test input can be vectorized using the same training input vocabulary, and the predicted results can be decoded using the same training output vocabulary."
      ],
      "metadata": {
        "id": "Xiy5o2NRcguC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the source_vectorization and target_vectorization\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import string\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "strip_chars = string.punctuation\n",
        "# print(type(strip_chars))\n",
        "\n",
        "strip_chars = strip_chars.replace(\"[\", \"\") # will keep [start] and [end] as it is to separate the marker from actual word start and end\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\") # replace punctuation characters with the empty string\n",
        "\n",
        "vocab_size = 100 \n",
        "sequence_length = 100 # max seq length seen in train data is 95\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "\n",
        "# load the train_pairs\n",
        "train_pairs_loaded = pickle.load(open('text_pairs', 'rb')) \n",
        "\n",
        "train_input_texts = [pair[0] for pair in train_pairs_loaded]\n",
        "train_output_texts = [pair[1] for pair in train_pairs_loaded]\n",
        "\n",
        "source_vectorization.adapt(train_input_texts)\n",
        "target_vectorization.adapt(train_output_texts)\n",
        "\n",
        "print(len(source_vectorization.get_vocabulary()))\n",
        "print(len(target_vectorization.get_vocabulary()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF9q7zciczSZ",
        "outputId": "5b536f23-2559-498d-bb5f-a33a39d9c74f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the `test_input` file and preprocess and translate it. Notice that the following decoding process would be long if the testing dataset is large."
      ],
      "metadata": {
        "id": "LXtNpv0Zm-o3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load and preprocess the testing input data\n",
        "\n",
        "def strip_markers(sentence):\n",
        "  # strip the leading [start] and tailing [end] if existing\n",
        "\n",
        "  words = sentence.split()\n",
        "  if words[0] == \"[start]\":\n",
        "    clean_words = words[1:]\n",
        "  else:\n",
        "    clean_words = words\n",
        "  \n",
        "  if clean_words[-1] == \"[end]\":\n",
        "    clean_words = clean_words[:-1]\n",
        "  else:\n",
        "    clean_words = clean_words\n",
        "\n",
        "  clean_text = \" \".join(clean_words)\n",
        "  return clean_text\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer_loaded(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "# load the test input data\n",
        "test_input = pickle.load(open('DS_5_test_input', 'rb')) \n",
        "\n",
        "# For self testing purposes\n",
        "# test_input_texts = [pair[0] for pair in train_pairs]\n",
        "# test_output_texts = [strip_markers(pair[1]) for pair in train_pairs]\n",
        "\n",
        "# test_input_texts = [pair[0] for pair in val_pairs]\n",
        "# test_output_texts = [strip_markers(pair[1]) for pair in val_pairs]\n",
        "\n",
        "# test_input_texts = [pair[0] for pair in train_pairs_loaded]\n",
        "# test_output_texts = [strip_markers(pair[1]) for pair in train_pairs_loaded]\n",
        "\n",
        "# test_input = test_input_texts[-20:]\n",
        "# test_output = test_output_texts[-20:]\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 100 # here we use max seq of 100\n",
        "\n",
        "# run the trained mode to get the predicted output\n",
        "test_pred = []\n",
        "for test_sentence in tqdm(test_input):\n",
        "  test_pred.append(strip_markers(decode_sequence(test_sentence)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU8WRl3mh3hS",
        "outputId": "e4e21aab-4970-496e-f7ec-96df0050b996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:54<00:00,  2.70s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To calculate the \"accuracy for words\""
      ],
      "metadata": {
        "id": "TLtDxKDXcjja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First load the `test_output`, then you can calculate the test accuracy using the \"accuracy for words\" metric (you can use your own or use my implementation below)"
      ],
      "metadata": {
        "id": "cwx7L7A374Bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_word_acc(target_texts, pred_texts):\n",
        "  total_ct = 0\n",
        "  match_ct = 0\n",
        "\n",
        "  assert(len(target_texts)==len(pred_texts)) # assert target sentence number match pred sentence number\n",
        "  \n",
        "  text_pairs = zip(target_texts, pred_texts) \n",
        "  for pair in text_pairs: # loop through each sentence\n",
        "    target = pair[0]\n",
        "    pred = pair[1]\n",
        "    print()\n",
        "    print(\"target:\", target)\n",
        "    print(\"predic:\", pred)\n",
        "    \n",
        "    target_words = target.split()\n",
        "    pred_words = pred.split()\n",
        "\n",
        "    total_ct += len(target_words)\n",
        "    min_len = min(len(target_words), len(pred_words))\n",
        "    print('target_len:', len(target_words))\n",
        "    print('pred_len:', len(pred_words))\n",
        "    \n",
        "    m_ct = 0\n",
        "    for i in range(min_len): # loop through each word\n",
        "      if target_words[i] == pred_words[i]:\n",
        "        m_ct += 1\n",
        "        match_ct += 1\n",
        "    \n",
        "    print('m_ct', m_ct)\n",
        "    print('match_ct', match_ct)\n",
        "    print('total_ct', total_ct)\n",
        "    \n",
        "  word_acc = match_ct / total_ct\n",
        "  return word_acc\n",
        "\n",
        "# load the test output data\n",
        "test_output = pickle.load(open('DS_5_test_output', 'rb'))\n",
        "\n",
        "acc = calculate_word_acc(test_output, test_pred)\n",
        "print(\"\\nTesting accuracy is\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2HfL43vh-BS",
        "outputId": "522a8cb4-482b-432d-8d2d-2c01524f0dd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "target: c g b g a e e b f a i d f g c g a k h i b f a g j k b f a k l m b e b f a i ed ee ef b g b g b d a h eh ei ej b g a i eg ek el c d a f em fd a e fe\n",
            "predic: c g b g a e e a k d f b f c g a i g h i b f a g j k b f a k l m b e b f a i ed ee ef b g b d b g a h eh ei ej b g a i eg ek el c d a f em fd a e fe\n",
            "target_len: 68\n",
            "pred_len: 68\n",
            "m_ct 56\n",
            "match_ct 56\n",
            "total_ct 68\n",
            "\n",
            "target: b e c g b d c d a h e f g b g c e a j h i j c e a f k l c g a g m ed b g a d ee ef b f a h d eg eh\n",
            "predic: b e c g b d c d a h e f g b g c e a j h i j c e a f k l c g a g m ed b g a d ee ef b f a h d eg eh\n",
            "target_len: 47\n",
            "pred_len: 47\n",
            "m_ct 47\n",
            "match_ct 103\n",
            "total_ct 115\n",
            "\n",
            "target: b g a e d c e c f a h e f g c e b f c e b d a d k l b f b g b f a k ee ef a j m ed eg a k j eh c d c f b g a e el b d a j ek em fd c e a h ej fe ff a j i ei fg c g a e fi a f fh fj a k h fk\n",
            "predic: b g a e d c e c f a h e f g b f c d b f b f a e l b d c f a k ed ee b g b e c d c g c e b f a j ej ek el a j eh ei em a k eg fd a k ef fe a k m ff c e a j j fg fh a f i fi a h f g fj b g a k fk fl\n",
            "target_len: 86\n",
            "pred_len: 89\n",
            "m_ct 20\n",
            "match_ct 123\n",
            "total_ct 201\n",
            "\n",
            "target: b d b f b f a k e f b g b g a g h i b e c f a f k l a i g j m b e b e c g c g b d a j eg eh ei a j ee ef ej a h d ed ek b g c d c d b e a i fd fe ff a e fg a h el em fh\n",
            "predic: b d b f b f a k e f b g b g a g h i b f a e k a i g j l c e b e b e c g c g a j ef eg eh a j ed ee ei a h d m ej b g b g c d b e a i em fd fe a e ff a h ek el fg b e c e a h fh fi fj\n",
            "target_len: 77\n",
            "pred_len: 83\n",
            "m_ct 21\n",
            "match_ct 144\n",
            "total_ct 278\n",
            "\n",
            "target: c f c d a g d e c e b g c e c e a i h i j b f a e l a f k m b e c f a i ed ee ef b f a e eh c e a d ei ej b d a j eg ek el a d g em a d f fd\n",
            "predic: c f c d a g d e c e b g c e b f a e j a i h i k b f a e m a f l ed b f c e a d ef eg b f a e ei a i ee eh ej c e b d a j ek el em a d g fd a d f fe\n",
            "target_len: 65\n",
            "pred_len: 68\n",
            "m_ct 17\n",
            "match_ct 161\n",
            "total_ct 343\n",
            "\n",
            "target: b g c g c e b g b f a e h a h f g i a f e j c g c g c f a d m ed a j k l ee a d d ef c f a g eg eh\n",
            "predic: b g c g b e b g c f a e h a h f g i a f e j c g c f c f a d m ed a j k l ee a d d ef c f a g eg eh\n",
            "target_len: 47\n",
            "pred_len: 47\n",
            "m_ct 44\n",
            "match_ct 205\n",
            "total_ct 390\n",
            "\n",
            "target: c g c g c e a k e f b f b d a j g h i b e c d c e b f b g c f a h ed ee ef c g a f eg eh a h l m ei a f k ej a h d j ek\n",
            "predic: c g c g c e a k e f b f b d a j g h i b e c d c f b f b g c f a h ed ee ef c g a f eg eh a h l m ei a f k ej a h d j ek\n",
            "target_len: 56\n",
            "pred_len: 56\n",
            "m_ct 55\n",
            "match_ct 260\n",
            "total_ct 446\n",
            "\n",
            "target: c e b g a e e b d b f b d c e a f i j b g a h h k l a e m a k g ed a d f ee b e a e eg a d ef eh c d c e a k ej ek c e b g b d b g a g fe ff a j em fd fg a f el fh b e a g fi fj b d a h ei fk fl c g a h d fm gd\n",
            "predic: c e b g a e e b d b f b e b d a f i j c e a h h k l a k g m a e ed a d g ee b e a e eg a d ef eh c d c e b g b g b d a g fd fe a j el em ff a f ek fg a f ej fh b g a j ei fi b e a d fj fk c g a e fl fm a h d gd\n",
            "target_len: 95\n",
            "pred_len: 95\n",
            "m_ct 42\n",
            "match_ct 302\n",
            "total_ct 541\n",
            "\n",
            "target: b e c f b d a d e f a e g b g a f h i c d b d a h j k l c d a f m ed a k d ee a e ef b g c f a h eg eh ei\n",
            "predic: b e c f b d a f e f a e g b g a f h i b d c d a h j k l b d a f m ed a k d ee a e ef b g c f a h eg eh ei\n",
            "target_len: 50\n",
            "pred_len: 50\n",
            "m_ct 46\n",
            "match_ct 348\n",
            "total_ct 591\n",
            "\n",
            "target: c d b f c e c g a e g c f c d c g a h i j k c d a f l m a k h ed a e ee a k f ef c f a f eg eh a k e ei c e a f ej ek a d d el\n",
            "predic: c d b f c e c g b g a e h c d c e a f j k c g a h i l m c f a f ed ee a k g ef a e eg a k f eh a k e ei c e a f ej ek a d d el\n",
            "target_len: 59\n",
            "pred_len: 59\n",
            "m_ct 30\n",
            "match_ct 378\n",
            "total_ct 650\n",
            "\n",
            "target: c e b f c e c f a g f g a i d e h c d b d b f c g c d a d m ed a e ee c g b g a k eg eh a i l ef ei a g k ej a g j ek a e el b e a i i em fd c g a g fe ff b f a g fg fh b e a g fi fj\n",
            "predic: c e b f c e c f a g f g a i d e h c g b d b e b f a e m c f a g ed ee c d a i l ef eg a g k eh a e ei a i i ej b e a g ek el b e a g em fd a g j fe c f a g ff fg b e a g fh fi b e a g fj fk\n",
            "target_len: 83\n",
            "pred_len: 86\n",
            "m_ct 28\n",
            "match_ct 406\n",
            "total_ct 733\n",
            "\n",
            "target: b g c f c e b g a g f g a k e h b f c e a f j k a k i l a f d m c e c d a h ed ee ef c d b g a g eh ei b d a j eg ej ek\n",
            "predic: b g c f b e b g a g f g a k e h c f b f a f j k a k i l a f d m c e c d a h ed ee ef c d b g a g eh ei b d a j eg ej ek\n",
            "target_len: 56\n",
            "pred_len: 56\n",
            "m_ct 52\n",
            "match_ct 458\n",
            "total_ct 789\n",
            "\n",
            "target: b d c f c f a k e f b f a k g h b e c g a e k b f b d c e a j m ed ee a h j l ef a d i eg a g d eh\n",
            "predic: b d c f c f a k e f b f a k g h b e c g a e k b f b d c e a j m ed ee a h j l ef a d i eg a g d eh\n",
            "target_len: 47\n",
            "pred_len: 47\n",
            "m_ct 47\n",
            "match_ct 505\n",
            "total_ct 836\n",
            "\n",
            "target: c f c d a e e b e b f b d b d a g i j b f a e l a k k m c f a e ee b e a e eg a i ed ef eh a k h ei c g b d a j ej ek el b e a d em fd c g a j g fe ff a i d f fg\n",
            "predic: c f c d a e e b e b f b d b d a g i j b f a e l a k k m c f a e ee b e a e eg a i ed ef eh a k h ei c g b d a j ej ek el b e a d em fd c g a j g fe ff a i d f fg\n",
            "target_len: 74\n",
            "pred_len: 74\n",
            "m_ct 74\n",
            "match_ct 579\n",
            "total_ct 910\n",
            "\n",
            "target: b g c d c e a h d e f c d c g b g a f i j b g a i h k l a k g m c e c d a k ee ef c f a e eh a f eg ei a d ed ej b f c f c e a k em fd a e fe c d a i el ff fg a k ek fh\n",
            "predic: b g c d c e a h d e f c d c g b g a f i j b g a i h k l a k g m c e c d a k ee ef c f a e ei a f eh ej a d eg ek c f c e a k em fd a f el fe a i ed ff a g h e fg c d a k fh fi\n",
            "target_len: 77\n",
            "pred_len: 80\n",
            "m_ct 50\n",
            "match_ct 629\n",
            "total_ct 987\n",
            "\n",
            "target: c f b d c f c d c g b d c f a k i j b g b g c f a i l m ed a g k ee b e a g ef eg a k h eh c f c g a e ek c e c d a k em fd a g el fe b d a j ej ff fg b e a j ei fh fi a i f g fj c f a f fk fl a h d e fm\n",
            "predic: b f c d c f c g c g b d c f a k i j b g b g c e a i l m ed a g k ee b e a g ef eg a k h eh c f c g a e ek c e c d a k em fd a g el fe b d a j ej ff fg b e a j ei fh fi a i f g fj c f a f fk fl a h d e fm\n",
            "target_len: 92\n",
            "pred_len: 92\n",
            "m_ct 88\n",
            "match_ct 717\n",
            "total_ct 1079\n",
            "\n",
            "target: c e b d c e c g c d c f c f c e a j i j k a i g h l a e m c e c f a e ef a d ee eg a e eh b d a h ed ei ej a d f ek b d a h e el em a f d fd\n",
            "predic: c e b d c e c g c f c f c f c e a j i j k a i g h l a e m c e c f a e ef a d ee eg a e eh b d a h ed ei ej a d f ek b d a h e el em a f d fd\n",
            "target_len: 65\n",
            "pred_len: 65\n",
            "m_ct 64\n",
            "match_ct 781\n",
            "total_ct 1144\n",
            "\n",
            "target: c f c g b d a k e f c g a e h a h d g i b g c f c e a k l m b d c f a f ee ef a j k ed eg a k j eh a e ei\n",
            "predic: c f c g b d a k e f c g a e h a h d g i b g c f c e a k l m b d c f a f ee ef a j k ed eg a k j eh a e ei\n",
            "target_len: 50\n",
            "pred_len: 50\n",
            "m_ct 50\n",
            "match_ct 831\n",
            "total_ct 1194\n",
            "\n",
            "target: c d b f b f b e b g a f g h a g f i c d a g j k b e b g c g a k ed ee a h l m ef a k e eg a k d eh\n",
            "predic: c d b f b f b e b g a f g h a g f i b d a g j k b e c g c g a k ed ee a h l m ef a k e eg a k d eh\n",
            "target_len: 47\n",
            "pred_len: 47\n",
            "m_ct 45\n",
            "match_ct 876\n",
            "total_ct 1241\n",
            "\n",
            "target: c d b g b e b e a f f g c g b d a g i j a h e h k a e l b e c g b f c f a i ee ef eg b d c f a i eh ei ej a d ed ek b g a g el em a k m fd a g d fe b f c f a i ff fg fh\n",
            "predic: c d b g b e b e a f f g c g b d a g i j a h e h k a e l b e c g b f c f a i ee ef eg b g a i ed eh ei b f a d ej ek c f a g el em a k m fd a g d fe b f c f a i ff fg fh\n",
            "target_len: 77\n",
            "pred_len: 77\n",
            "m_ct 66\n",
            "match_ct 942\n",
            "total_ct 1318\n",
            "\n",
            "Testing accuracy is 0.7147192716236722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For grader"
      ],
      "metadata": {
        "id": "FJOnV3yZLHM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Step1: \n",
        "\n",
        "upload the attached `text_pairs` and `tianliu_525004380_project2_transformer_final_0.9290.weights.data-00000-of-00001` and `tianliu_525004380_project2_transformer_final_0.9290.weights.index` to your working directory"
      ],
      "metadata": {
        "id": "atLxtr9hLsjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Step 2: \n",
        "\n",
        "The followinng code block does the following:\n",
        "* rebuild source and target vectorization from `text_pairs`\n",
        "* load the pretrained model by loading its weights\n",
        "* make prediction\n",
        "* calculate accuracy. \n",
        "\n",
        "Please change the two lines of code to your `test_input` and `test_out`. Then run the following code block to get accuracy."
      ],
      "metadata": {
        "id": "6eIOe_h-L7WB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_input = 'grader_test_input'\n",
        "# test_out = 'grader_test_output'\n",
        "\n",
        "# model = keras.models.load_model(\"Your model\")\n",
        "# Fill in your code here, to the variable 'pre', without re-training\n",
        "\n",
        "#-------------------------------My code\n",
        "\n",
        "# initialize the source_vectorization and target_vectorization from `text_pairs`\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import string\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "strip_chars = string.punctuation\n",
        "# print(type(strip_chars))\n",
        "strip_chars = strip_chars.replace(\"[\", \"\") # will keep [start] and [end] as it is to separate the marker from actual word start and end\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\") # replace punctuation characters with the empty string\n",
        "\n",
        "vocab_size = 100 \n",
        "sequence_length = 100 # max seq length seen in train data is 95\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "# load the train_pairs\n",
        "train_pairs_loaded = pickle.load(open('text_pairs', 'rb')) \n",
        "\n",
        "train_input_texts = [pair[0] for pair in train_pairs_loaded]\n",
        "train_output_texts = [pair[1] for pair in train_pairs_loaded]\n",
        "\n",
        "source_vectorization.adapt(train_input_texts)\n",
        "target_vectorization.adapt(train_output_texts)\n",
        "print(\"source and target vectorization rebuilt.\")\n",
        "\n",
        "print(len(source_vectorization.get_vocabulary()))\n",
        "print(len(target_vectorization.get_vocabulary()))\n",
        "\n",
        "#---------------------\n",
        "\n",
        "# import pickle\n",
        "# # load the test input data\n",
        "# test_input = pickle.load(open('DS_5_train_input', 'rb'))\n",
        "# test_input = test_input[:10]\n",
        "# test_output = pickle.load(open('DS_5_train_output', 'rb'))\n",
        "# test_out = test_output[:10]\n",
        "\n",
        "# load the model by loading the saved weights\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions # add up the word embedding and position embedding\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        \n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        \n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        \n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        \n",
        "        return self.layernorm_3(attention_output_2 + proj_output)\n",
        "\n",
        "embed_dim = 128\n",
        "dense_dim = 512\n",
        "num_heads = 8\n",
        "vocab_size = 100\n",
        "sequence_length = 100\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"input\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"output\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "transformer_loaded = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# load the saved weights\n",
        "transformer_loaded.load_weights('tianliu_525004380_project2_transformer_final_0.9290.weights')\n",
        "print(\"trained model loaded.\")\n",
        "\n",
        "# make prediction\n",
        "def strip_markers(sentence):\n",
        "  # strip the leading [start] and tailing [end] if existing\n",
        "\n",
        "  words = sentence.split()\n",
        "  if words[0] == \"[start]\":\n",
        "    clean_words = words[1:]\n",
        "  else:\n",
        "    clean_words = words\n",
        "  \n",
        "  if clean_words[-1] == \"[end]\":\n",
        "    clean_words = clean_words[:-1]\n",
        "  else:\n",
        "    clean_words = clean_words\n",
        "\n",
        "  clean_text = \" \".join(clean_words)\n",
        "  return clean_text\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer_loaded(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 100 # here we use max seq of 100\n",
        "\n",
        "# run the trained mode to get the predicted output\n",
        "print(\"makinng predictions in test_input ......\")\n",
        "test_pred = []\n",
        "for test_sentence in tqdm(test_input):\n",
        "  test_pred.append(strip_markers(decode_sequence(test_sentence)))\n",
        "\n",
        "pre = test_pred\n",
        "print(\"\\n\\npredictions done.\")\n",
        "# pre = your_decoded_list_of_output_strings\n",
        "\n",
        "#------------------------------- continue with grader's code, i have to modify the index_range to handle the case when the predictions is shorter than groundtruth\n",
        "\n",
        "total = 0\n",
        "correct = 0\n",
        "for sen in range(len(test_out)):\n",
        "    temp = test_out[sen].split()\n",
        "    guess = pre[sen].split()\n",
        "    # print(sen)\n",
        "    # print(temp)\n",
        "    # print(guess)\n",
        "    index_range = min(len(temp), len(guess)) # i changed here\n",
        "    for index in range(index_range): # i changed here\n",
        "        cor = temp[index]\n",
        "        gus = guess[index]\n",
        "        if gus == cor:\n",
        "            correct += 1\n",
        "    total += len(temp) # i changed here\n",
        "print(total)\n",
        "print(correct)\n",
        "acc = correct / total\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-nyq1bvLKDc",
        "outputId": "dae4f6fe-d69f-42d6-d0d6-c07c429b9dfd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source and target vectorization rebuilt.\n",
            "13\n",
            "38\n",
            "trained model loaded.\n",
            "makinng predictions in test_input ......\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.momentum\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.rho\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-4.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-4.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-0.token_embeddings.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-0.position_embeddings.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-1.token_embeddings.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-1.position_embeddings.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.layernorm_1.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.layernorm_1.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.layernorm_2.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.layernorm_2.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.layernorm_1.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.layernorm_1.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.layernorm_2.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.layernorm_2.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.layernorm_3.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.layernorm_3.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.attention._query_dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.attention._query_dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.attention._key_dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.attention._key_dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.attention._value_dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.attention._value_dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.attention._output_dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.attention._output_dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.dense_proj.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.dense_proj.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.dense_proj.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-2.dense_proj.layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.attention_1._query_dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.attention_1._query_dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.attention_1._key_dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.attention_1._key_dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.attention_1._value_dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.attention_1._value_dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.attention_1._output_dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.attention_1._output_dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.attention_2._query_dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.attention_2._query_dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.attention_2._key_dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.attention_2._key_dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.attention_2._value_dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.attention_2._value_dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.attention_2._output_dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.attention_2._output_dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.dense_proj.layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.dense_proj.layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.dense_proj.layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'rms' for (root).layer_with_weights-3.dense_proj.layer_with_weights-1.bias\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:17<00:00,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predictions done.\n",
            "0\n",
            "['b', 'f', 'c', 'f', 'b', 'f', 'c', 'd', 'a', 'j', 'e', 'f', 'g', 'c', 'e', 'b', 'g', 'a', 'k', 'i', 'j', 'b', 'd', 'b', 'f', 'a', 'k', 'l', 'm', 'b', 'f', 'b', 'd', 'a', 'h', 'ed', 'ee', 'ef', 'a', 'k', 'k', 'eg', 'a', 'k', 'h', 'eh', 'a', 'e', 'ei', 'c', 'd', 'a', 'f', 'ej', 'ek', 'a', 'g', 'd', 'el']\n",
            "['b', 'f', 'c', 'f', 'b', 'f', 'c', 'd', 'a', 'j', 'e', 'f', 'g', 'c', 'e', 'b', 'g', 'a', 'k', 'i', 'j', 'b', 'd', 'b', 'f', 'a', 'k', 'l', 'm', 'b', 'f', 'b', 'f', 'a', 'h', 'ed', 'ee', 'ef', 'a', 'k', 'k', 'eg', 'a', 'k', 'h', 'eh', 'a', 'e', 'ei', 'c', 'd', 'a', 'f', 'ej', 'ek', 'a', 'g', 'd', 'el']\n",
            "1\n",
            "['b', 'e', 'c', 'd', 'b', 'g', 'c', 'g', 'a', 'i', 'e', 'f', 'g', 'a', 'f', 'd', 'h', 'c', 'e', 'c', 'f', 'c', 'f', 'a', 'e', 'l', 'c', 'd', 'a', 'i', 'k', 'm', 'ed', 'a', 'i', 'i', 'j', 'ee', 'a', 'e', 'ef', 'a', 'e', 'eg', 'a', 'e', 'eh', 'c', 'd', 'a', 'k', 'ei', 'ej']\n",
            "['b', 'f', 'a', 'e', 'd', 'c', 'd', 'b', 'g', 'c', 'g', 'a', 'i', 'f', 'g', 'h', 'a', 'f', 'e', 'i', 'c', 'e', 'c', 'f', 'c', 'd', 'a', 'i', 'k', 'l', 'm', 'c', 'f', 'a', 'e', 'ee', 'c', 'd', 'a', 'i', 'ed', 'ef', 'eg', 'a', 'k', 'j', 'eh', 'c', 'd', 'a', 'k', 'ei', 'ej']\n",
            "2\n",
            "['c', 'f', 'b', 'e', 'c', 'd', 'b', 'f', 'b', 'e', 'a', 'k', 'g', 'h', 'b', 'e', 'a', 'k', 'i', 'j', 'b', 'd', 'c', 'd', 'a', 'h', 'k', 'l', 'm', 'c', 'd', 'a', 'e', 'ee', 'a', 'h', 'f', 'ed', 'ef', 'a', 'g', 'e', 'eg', 'b', 'f', 'a', 'd', 'eh', 'ei', 'a', 'k', 'd', 'ej']\n",
            "['c', 'f', 'b', 'e', 'c', 'd', 'b', 'f', 'b', 'e', 'a', 'k', 'g', 'h', 'b', 'e', 'a', 'k', 'i', 'j', 'b', 'd', 'c', 'e', 'a', 'h', 'k', 'l', 'm', 'c', 'f', 'a', 'e', 'ee', 'a', 'h', 'f', 'ed', 'ef', 'a', 'g', 'e', 'eg', 'b', 'f', 'a', 'd', 'eh', 'ei', 'a', 'k', 'd', 'ej']\n",
            "3\n",
            "['b', 'g', 'b', 'e', 'c', 'g', 'a', 'g', 'e', 'f', 'c', 'g', 'a', 'j', 'd', 'g', 'h', 'b', 'g', 'b', 'g', 'b', 'f', 'a', 'g', 'k', 'l', 'b', 'e', 'a', 'k', 'm', 'ed', 'a', 'j', 'i', 'j', 'ee', 'c', 'e', 'c', 'e', 'a', 'h', 'ef', 'eg', 'eh']\n",
            "['b', 'g', 'b', 'e', 'c', 'g', 'a', 'g', 'e', 'f', 'b', 'g', 'a', 'j', 'd', 'g', 'h', 'b', 'g', 'b', 'f', 'a', 'g', 'j', 'k', 'b', 'e', 'a', 'k', 'l', 'm', 'c', 'e', 'a', 'k', 'ed', 'ee', 'c', 'e', 'a', 'h', 'ef', 'eg', 'c', 'e', 'a', 'h', 'i', 'eh', 'ei']\n",
            "4\n",
            "['b', 'd', 'c', 'g', 'c', 'e', 'a', 'f', 'e', 'f', 'c', 'd', 'a', 'd', 'g', 'h', 'c', 'f', 'c', 'f', 'a', 'j', 'i', 'j', 'k', 'a', 'e', 'l', 'c', 'f', 'a', 'k', 'm', 'ed', 'c', 'd', 'c', 'd', 'a', 'f', 'ef', 'eg', 'b', 'g', 'c', 'g', 'a', 'g', 'ei', 'ej', 'c', 'e', 'a', 'h', 'eh', 'ek', 'el', 'a', 'g', 'ee', 'em', 'b', 'd', 'c', 'e', 'a', 'h', 'fd', 'fe', 'ff', 'a', 'k', 'd', 'fg']\n",
            "['b', 'd', 'c', 'g', 'c', 'e', 'a', 'f', 'e', 'f', 'c', 'd', 'a', 'g', 'g', 'h', 'c', 'f', 'c', 'f', 'a', 'j', 'i', 'j', 'k', 'a', 'e', 'l', 'c', 'f', 'a', 'k', 'm', 'ed', 'c', 'd', 'c', 'd', 'b', 'g', 'a', 'g', 'eh', 'ei', 'a', 'f', 'eg', 'ej', 'a', 'g', 'ef', 'ek', 'c', 'e', 'b', 'd', 'a', 'h', 'el', 'em', 'c', 'e', 'a', 'h', 'ee', 'fd', 'fe', 'c', 'e', 'a', 'g', 'ff', 'fg', 'a', 'k', 'd', 'fh']\n",
            "5\n",
            "['c', 'g', 'b', 'g', 'b', 'd', 'c', 'f', 'c', 'g', 'a', 'e', 'h', 'a', 'f', 'g', 'i', 'a', 'd', 'f', 'j', 'c', 'e', 'a', 'f', 'k', 'l', 'c', 'd', 'c', 'e', 'a', 'e', 'ee', 'a', 'g', 'ed', 'ef', 'a', 'd', 'm', 'eg', 'b', 'd', 'a', 'j', 'e', 'eh', 'ei', 'a', 'k', 'd', 'ej']\n",
            "['c', 'g', 'b', 'g', 'b', 'f', 'c', 'f', 'c', 'g', 'a', 'e', 'h', 'a', 'g', 'g', 'i', 'a', 'd', 'f', 'j', 'c', 'g', 'a', 'e', 'l', 'a', 'f', 'k', 'm', 'c', 'd', 'a', 'e', 'ee', 'a', 'd', 'ed', 'ef', 'c', 'e', 'a', 'j', 'e', 'eg', 'eh', 'a', 'k', 'd', 'ei']\n",
            "6\n",
            "['c', 'e', 'c', 'f', 'b', 'e', 'a', 'd', 'e', 'f', 'c', 'd', 'c', 'g', 'b', 'e', 'c', 'f', 'c', 'd', 'a', 'k', 'k', 'l', 'a', 'h', 'i', 'j', 'm', 'a', 'h', 'g', 'h', 'ed', 'a', 'e', 'ee', 'a', 'g', 'd', 'ef', 'c', 'd', 'b', 'g', 'a', 'h', 'eg', 'eh', 'ei']\n",
            "['c', 'e', 'c', 'f', 'b', 'e', 'a', 'd', 'e', 'f', 'c', 'd', 'b', 'g', 'c', 'f', 'c', 'f', 'a', 'k', 'j', 'k', 'c', 'd', 'a', 'h', 'i', 'l', 'm', 'a', 'h', 'g', 'h', 'ed', 'a', 'e', 'ee', 'a', 'g', 'd', 'ef', 'c', 'd', 'b', 'g', 'a', 'h', 'eg', 'eh', 'ei']\n",
            "7\n",
            "['c', 'e', 'c', 'g', 'b', 'e', 'a', 'e', 'f', 'a', 'e', 'g', 'a', 'd', 'e', 'h', 'c', 'e', 'a', 'd', 'i', 'j', 'a', 'd', 'd', 'k', 'a', 'e', 'l', 'c', 'f', 'b', 'f', 'c', 'd', 'c', 'f', 'a', 'k', 'ef', 'eg', 'b', 'f', 'a', 'i', 'ee', 'eh', 'ei', 'a', 'i', 'm', 'ed', 'ej']\n",
            "['c', 'e', 'c', 'g', 'b', 'e', 'a', 'e', 'f', 'a', 'e', 'g', 'a', 'd', 'e', 'h', 'c', 'e', 'a', 'd', 'i', 'j', 'a', 'd', 'd', 'k', 'a', 'e', 'l', 'b', 'f', 'c', 'd', 'c', 'f', 'a', 'k', 'ee', 'ef', 'c', 'f', 'b', 'f', 'a', 'i', 'eg', 'eh', 'ei', 'a', 'i', 'm', 'ed', 'ej']\n",
            "8\n",
            "['b', 'g', 'a', 'e', 'd', 'b', 'd', 'b', 'f', 'c', 'f', 'a', 'h', 'f', 'g', 'h', 'a', 'e', 'i', 'a', 'e', 'j', 'b', 'f', 'b', 'f', 'a', 'k', 'l', 'm', 'a', 'd', 'k', 'ed', 'a', 'd', 'e', 'ee', 'c', 'g', 'a', 'e', 'eg', 'a', 'k', 'ef', 'eh']\n",
            "['b', 'g', 'a', 'e', 'd', 'b', 'd', 'b', 'f', 'c', 'f', 'a', 'h', 'f', 'g', 'h', 'a', 'e', 'i', 'a', 'e', 'j', 'b', 'f', 'b', 'f', 'a', 'k', 'l', 'm', 'a', 'd', 'k', 'ed', 'a', 'd', 'e', 'ee', 'c', 'g', 'a', 'e', 'eg', 'a', 'k', 'ef', 'eh']\n",
            "9\n",
            "['b', 'e', 'a', 'e', 'd', 'b', 'e', 'a', 'g', 'e', 'f', 'b', 'g', 'b', 'f', 'a', 'd', 'h', 'i', 'c', 'f', 'b', 'g', 'a', 'k', 'k', 'l', 'c', 'e', 'a', 'i', 'j', 'm', 'ed', 'b', 'g', 'a', 'e', 'ef', 'b', 'd', 'a', 'h', 'ee', 'eg', 'eh', 'b', 'e', 'c', 'e', 'a', 'd', 'ej', 'ek', 'a', 'd', 'ei', 'el', 'a', 'd', 'g', 'em']\n",
            "['b', 'e', 'a', 'e', 'd', 'b', 'e', 'a', 'g', 'e', 'f', 'b', 'g', 'b', 'f', 'a', 'd', 'h', 'i', 'c', 'f', 'b', 'e', 'a', 'k', 'k', 'l', 'a', 'g', 'j', 'm', 'b', 'g', 'a', 'e', 'ee', 'b', 'd', 'a', 'h', 'ed', 'ef', 'eg', 'b', 'e', 'c', 'e', 'a', 'd', 'ei', 'ej', 'c', 'e', 'a', 'h', 'eh', 'ek', 'el', 'a', 'd', 'g', 'em']\n",
            "551\n",
            "359\n",
            "0.6515426497277677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}